ABSTRACT

Emotions are mental states that result from neurophysiological changes and can be associated with thoughts, feelings, behavior, and a degree of pleasure or displeasure. While face detection technology has been around for a while, the ability to detect emotions being displayed on the face is now possible. This technology is becoming increasingly important for gauging the emotions and reactions of people in a crowd, which can be useful for making informed decisions related to intent identification, promotion of offers, and security-related threats. Emotion recognition based on facial expression is a subfield of social signal processing which enables a wide range of applications, specifically in human- computer interaction. However, recognizing simple emotions such as anger, happiness, contempt, fear, sadness, and surprise remains a challenging problem in computer vision. Deep learning techniques, particularly Convolutional Neural Networks (CNN), have recently been useful in solving real-world problems, including emotion recognition. In this project, we implemented CNN-based deep learning techniques to analyze facial expressions and detect the emotions being displayed by individuals.


INTRODUCTION

Facial expression recognition using computer vision and machine learning has become a crucial research area with various applications in security, human-computer interaction, driver safety, and health care. However, recognizing facial expressions accurately is challenging due to variations across the human population and context-dependent variations. Automatic facial expression recognition has numerous benefits such as identifying human mentalities without asking them, evaluating stress levels, burnouts, etc. Emotion recognition is used in a wide range of applications, including business promotions, clinical psychology, psychiatry, neurology, pain assessment, lie detection, intelligent settings, and multimodal human-computer interface.
Despite significant advances in the field over the past decade, several limitations still exist, including inconsistent or absent reporting of inter-observer reliability and validity of expression metadata, common performance metrics, and standard protocols for common databases. However, using neural networks and machine learning algorithms, facial expressions can be classified, and patterns can be recognized with good accuracy. The proposed system utilizes Convolutional Neural Networks and a dataset with seven classes, including fear, contempt, disgust and anger, surprise, sadness, happiness, and neutral, which contains 28,821 images used for the purpose of training and 7,066 used for testing. This model is then designed, trained, saved, and further loaded for the testing process, where it captures live facial expressions via the system's default camera and predicts the output.

LITERATURE SURVEY

This survey provides an analysis of several related works in the field of emotion identification from facial images.
A hybrid optimization-based recurrent fuzzy neural network was proposed by Velagapudi et al. for the recognition of collective emotions from video sequences. In order to identify people's emotions from a group of them, the authors used neural networks. They employed MLTP, GLCM, and LESH to extract the face features from the source video frames and then used those features as inputs for their processing. They recommended a strategy for RFNN classifier optimisation that uses SSD to update the RFNN weights. They developed a method that outperformed existing deep learning algorithms with accuracy, recall, and precision of 99.16%, 99.33%, and 99%, respectively. With low-resolution images, they also achieved accuracy of 87.8%, recall of 88%, specificity of 87.7%, precision of 87.7%, and sensitivity of 88%. However, as the data was sourced from YouTube, the complexity may increase when compared to more comprehensive data sets.
Mayur et al. proposed a technique for identifying emotion from various facial photos using position, occlusion, and illumination. The model can adjust to all fluctuations in illumination, color, contrast, and head postures even though it was trained on a dataset with only stationary head poses and illuminations. Their hybrid approach produced better results than conventional machine learning models. Additionally, their hybrid model performed well on publicly accessible datasets such as EMOTIC, FER13, and FERG even with fewer training sets.
Wang et al. proposed that the accuracy of facial recognition using AAM combined with neural networks is higher than that of the point correlation method, but the point correlation method provides faster recognition time. They suggested using better minimization techniques, such as second-order minimization, to improve the accuracy of the recognition task, but the algorithm will take longer to converge. The experiment was carried out using a person-specific AAM model. They recommended using a large number of training textures and shapes to expand the system's ability to recognize different people.
Clavel et al. investigated the development of a fear-type emotion recognition system for a specific application, audio-video surveillance. They used fiction material to illustrate emotional manifestations in situations, including fear-type emotions. The fear vs. neutral classification achieved a mean accuracy rate of 71%, which is a promising result, given the variety of fear manifestations depicted in the SAFE Corpus. The best results were obtained during segments that occur during immediate threats.
Active Appearance Models (AAMs) and a linear support vector machine (SVM) classifier were used by Lucey et al. to give baseline results. SVMs look for the hyperplane that maximizes the difference between observations that are positive and those that are negative for a given class. By measuring the size and appearance of AAM characteristics, they were able to follow the patient's face. They deduced from the results that all feature types had very high overall accuracy, with the SPTS+CAPP feature extraction combination providing the best results. This suggests that both shape and appearance aspects include additional information.
Michael et al. attempted to use Gabor wavelets to code facial expressions, which is a novel approach. They coded images using a multi-orientation, multiresolution set of Gabor filters, and viewers rated the emotional content of each image using emotion adjectives to help evaluate the model. The similarities in facial expressions detected using Gabor coding and semantic similarity computed from human ratings were compared according to their respective ranks. They found that it is easier to evaluate the model based on posed expressions rather than the dataset.
Zhang et al. experimented with feature-based facial expression identification using a two-layer perceptron design. It was intended to create a recognition system that excels at using the training data.


PROPOSED METHODOLOGY

The proposed system utilizes computer vision to detect the emotions of people in a crowd. A convolutional neural network (CNN) is used to identify crowd emotions, expressed through facial expression changes. The system automatically detects the crowd count and provides a comprehensive analysis of the facial expressions, with the ability to generate immediate alerts if suspicious activities occur. The facial expression recognition is accomplished using deep neural networks, OpenCV, and Cascade Classifier, with images of individuals captured from various angles and with varying emotions as input data. The live capture is used as input to the model, which creates frames around each face using OpenCV and then utilizes the CNN model to detect emotions in the faces. The detected faces are compared to the trained data provided at the time of registration.
Machine learning algorithms, especially pattern recognition and classification, have proven to be highly useful. The feature extraction process is a critical aspect of these algorithms. This paper focuses on feature extraction and modification for algorithms such as CNN and OpenCV. Different algorithms and feature extraction techniques are compared to analyze their effectiveness in emotion detection. The human emotion dataset is used as an example to study the nature and robustness of classification algorithms and their performance for various datasets. Typically, face detection algorithms are applied to the image or captured frame before feature extraction for emotion detection.

This study suggests a hybrid approach for identifying emotions in facial picture collections. The addition of neurons and a smoother flow in the networks improves many hybrid-based systems, enabling them to classify large numbers of real-world datasets. The results show that even small layers can perform well, even with small datasets. The results of the proposed system are compared to those of existing systems using different publicly available datasets.
Our proposed system utilizes deep face recognition, a type of artificial intelligence, to recognize emotions in live feeds or videos. The system eliminates the problem of posed images by using a dataset of the same person taken from various angles, with all attributes marked for each person.
Deep neural networks, OpenCV, and Cascade Classifier are used to recognize facial expressions. The dataset for the model is made up of images of each person taken from various perspectives and expressions. The input to the model is live capture meant to obtain a live feed. After the input is passed, the model is triggered, and it begins processing by utilizing OpenCV to build frames around each face. The CNN model is then trained and used to detect emotions in the faces, while Haar cascade classifier is used to identify the faces in the feed. The detected faces are then compared to the trained data provided at the time of registration, and the labels are returned when the faces match. The system then focuses on classifying emotions for all individuals in the frame periodically. Emotions are detected in real-time and displayed on an output box. The entire detection process is fast, resulting in no community spread and faster, cost-effective results.

Convolutional Neural Networks (CNNs) are a type of Deep Learning Algorithm that can analyze images by identifying and assigning importance to various objects within them. If we were to consider other
 
classification algorithms, CNNs require less pre-processing, given they have the ability to learn filters and characteristics on their own, rather than relying on hand-engineered ones. Furthermore, CNNs are fully connected feed-forward neural networks, allowing them to efficiently analyze images and identify unique features within them.

CNNs are highly effective at reducing the number of parameters required for a model while still maintaining its quality. The CNN formula is composed of three fundamental steps: convolutional layer, pooling layer, and fully connected layer. In the convolutional layer, a set of filters or kernels is applied to extract features from the input image, and the resulting feature map is calculated using element- wise multiplication and addition. The pooling layer then down samples the feature map to reduce its dimensionality using techniques such as max pooling or average pooling. Finally, the fully connected layer links every neuron in the previous layer to every neuron in the following layer, conducting either classification or regression tasks. The output of the fully connected layer represents the predicted class label, which is determined by the index in the output tensor with the highest probability.

The convolutional layer is a critical component of the CNN formula, performing the primary function of convolution. A feature map that reflects the picture's learnt features is produced by sliding a window across the input image and using a filter to multiply and add elements one at a time. Non-linearity is incorporated into the network by passing the output of each layer through an activation function. The fully connected layer then performs classification or regression operations using the learned features.
Images have high dimensions, with each pixel considered a feature, making CNNs particularly useful for image analysis tasks. In this study, a crowd facial emotion recognition process was implemented using OpenCV, Cascade Classifier, and deep neural networks. Live capture was used as input, along with photographs of individuals taken from various angles and with different emotional expressions. We made use of the CNN model which was trained and applied to detect emotions in faces, which were then compared to the registered data. The detected faces were then labeled, and the emotions of all individuals in the frame were classified and displayed in real time.
This entire detection process is fast, affordable, and reduces the risk of community spread. The dataset used in this study was collected from the Kaggle website, which contained 28,821 images used for training and 7,066 for testing, with each image being 48x48 pixels in size. The dataset consisted of seven classes: Anger, Happy, Sad, Neutral, Disgust, Surprise, and Fear


CONCLUSION

Crowd emotion detection is a field that is growing exponentially and has the potential to make a significant impact on various industries, name a few - advertising, security, and entertainment. The ability to analyze and understand the emotion of a crowd in real-time can provide valuable insights into the behavior and reaction of a large group of people. However, practical aspects such as data privacy, accuracy, and implementing crowd emotion recognition systems. In this particular project, we have designed and programmed the model with multiple layers of convolutional neural networks along with the dataset used in FER13. The accuracy rate was 94% with 100 epochs giving the best results and meeting the expectation. This model would be extremely useful for analyzing the feedback of customers or customer satisfaction, and can be also used in classrooms to know if students have understood the session or not. Additionally, it is important to note that the field is still in its infancy, and further research is needed to improve the performance and reliability of this system. 